{% load static %}
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Building an autonomous self-driving car prototype with computer vision and deep learning">
  <title>Autonomous Self-Driving Car: From Concept to Reality</title>
  <style>
    :root {
      --primary-color: #2980b9;
      --secondary-color: #3498db;
      --text-color: #333;
      --background-color: #fff;
      --header-color: #2c3e50;
      --border-color: #eee;
      --highlight-bg: #f0f7fd;
      --highlight-border: #d0e3f0;
      --tag-bg: #e0f2f1;
      --tag-color: #00695c;
      --caption-color: #666;
    }

    [data-theme="dark"] {
      --primary-color: #3498db;
      --secondary-color: #2980b9;
      --text-color: #f0f0f0;
      --background-color: #121212;
      --header-color: #f5f5f5;
      --border-color: #333;
      --highlight-bg: #1e3a5f;
      --highlight-border: #2d4b7a;
      --tag-bg: #1e3d36;
      --tag-color: #4db6ac;
      --caption-color: #aaa;
    }

    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      line-height: 1.7;
      max-width: 900px;
      margin: 0 auto;
      padding: 20px;
      color: var(--text-color);
      background-color: var(--background-color);
      transition: background-color 0.3s, color 0.3s;
      position: relative;
    }

    header {
      margin-bottom: 40px;
    }

    h1 {
      color: var(--header-color);
      font-size: 2.2rem;
      margin-bottom: 15px;
      line-height: 1.3;
    }

    h2 {
      color: var(--primary-color);
      margin-top: 40px;
      border-bottom: 1px solid var(--border-color);
      padding-bottom: 8px;
    }

    h3 {
      color: var(--secondary-color);
      margin-top: 25px;
    }

    p {
      margin-bottom: 15px;
    }

    a {
      color: var(--secondary-color);
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    blockquote {
      background-color: rgba(41, 128, 185, 0.1);
      border-left: 4px solid var(--primary-color);
      padding: 15px 20px;
      margin: 25px 0;
      font-style: italic;
      color: var(--text-color);
    }

    ul, ol {
      margin-bottom: 20px;
      padding-left: 25px;
    }

    li {
      margin-bottom: 8px;
    }

    .highlight-box {
      background-color: var(--highlight-bg);
      border: 1px solid var(--highlight-border);
      border-radius: 4px;
      padding: 15px;
      margin: 20px 0;
    }

    .meta {
      color: var(--caption-color);
      font-size: 0.9rem;
      margin-bottom: 30px;
    }

    .tags {
      margin-top: 40px;
      display: flex;
      flex-wrap: wrap;
      gap: 5px;
    }

    .tag {
      display: inline-block;
      background-color: var(--tag-bg);
      color: var(--tag-color);
      padding: 3px 8px;
      border-radius: 3px;
      font-size: 0.8rem;
    }

    .conclusion {
      background-color: rgba(249, 249, 249, 0.1);
      padding: 20px;
      border-radius: 5px;
      margin-top: 30px;
    }

    .image-container {
      margin: 30px 0;
      text-align: center;
    }

    .image-container img {
      max-width: 100%;
      height: auto;
      border-radius: 5px;
      box-shadow: 0 3px 10px rgba(0,0,0,0.2);
    }

    .caption {
      font-size: 0.9rem;
      color: var(--caption-color);
      margin-top: 8px;
      font-style: italic;
    }

    .image-grid {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      gap: 15px;
      margin: 25px 0;
    }

    .image-grid img {
      width: 100%;
      height: auto;
    }

    /* Button Container Styles */
    .button-container {
      position: fixed;
      top: 0;
      right: 0;
      z-index: 999;
      display: flex;
      flex-direction: column;
      gap: 5px;
      padding: 5px;
      background: rgba(255,255,255,0.8);
      border-radius: 0 0 0 6px;
      box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      transition: background 0.3s;
    }

    [data-theme="dark"] .button-container {
      background: rgba(30,30,30,0.8);
    }

    /* GitHub Button Styles */
    .github-button {
      display: inline-flex;
      align-items: center;
      padding: 8px 12px;
      background-color: #24292e;
      color: white;
      border-radius: 4px;
      font-weight: 600;
      font-size: 14px;
      text-decoration: none;
      transition: all 0.2s;
    }

    .github-button:hover {
      background-color: #2d3338;
      transform: translateY(-2px);
    }

    .github-button svg {
      margin-right: 8px;
      fill: white;
      width: 16px;
      height: 16px;
    }

    /* Kaggle Button Styles */
    .kaggle-button {
      display: inline-flex;
      align-items: center;
      padding: 8px 12px;
      background-color: #20BEFF;
      color: white;
      border-radius: 4px;
      font-weight: 600;
      font-size: 14px;
      text-decoration: none;
      transition: all 0.2s;
    }

    .kaggle-button:hover {
      background-color: #0096D6;
      transform: translateY(-2px);
    }

    .kaggle-button svg {
      margin-right: 8px;
      fill: white;
      width: 16px;
      height: 16px;
    }

    [data-theme="dark"] .github-button {
      background-color: #f0f0f0;
      color: #24292e;
    }

    [data-theme="dark"] .github-button svg {
      fill: #24292e;
    }

    [data-theme="dark"] .github-button:hover {
      background-color: #e0e0e0;
    }

    [data-theme="dark"] .kaggle-button {
      background-color: #0096D6;
    }

    [data-theme="dark"] .kaggle-button:hover {
      background-color: #0077B5;
    }
    /* Colab Button Styles */
.colab-button {
  display: inline-flex;
  align-items: center;
  padding: 8px 12px;
  background-color: #F9AB00;
  color: black;
  border-radius: 4px;
  font-weight: 600;
  font-size: 14px;
  text-decoration: none;
  transition: all 0.2s;
}

.colab-button:hover {
  background-color: #f57c00;
  transform: translateY(-2px);
}

.colab-button svg {
  margin-right: 8px;
  fill: black;
  width: 16px;
  height: 16px;
}

[data-theme="dark"] .colab-button {
  background-color: #fbc02d;
  color: #121212;
}

[data-theme="dark"] .colab-button svg {
  fill: #121212;
}

[data-theme="dark"] .colab-button:hover {
  background-color: #f9a825;
}

    /* Theme Toggle Button */
    .theme-toggle {
      position: fixed;
      bottom: 20px;
      right: 20px;
      background: var(--primary-color);
      color: white;
      border: none;
      border-radius: 50%;
      width: 50px;
      height: 50px;
      font-size: 20px;
      cursor: pointer;
      display: flex;
      align-items: center;
      justify-content: center;
      box-shadow: 0 2px 10px rgba(0,0,0,0.2);
      z-index: 100;
    }

    /* Responsive adjustments */
    @media (max-width: 768px) {
      .github-button,
      .kaggle-button {
        padding: 6px 10px;
        font-size: 12px;
      }
      .github-button svg,
      .kaggle-button svg {
        width: 14px;
        height: 14px;
      }
    }
    /* Add to your style section */
.gif-container {
  margin: 25px 0;
  text-align: center;
}

.gif-container img {
  max-width: 100%;
  border-radius: 5px;
  box-shadow: 0 3px 10px rgba(0,0,0,0.2);
  border: 1px solid var(--border-color);
}

/* For autoplay controls */
.gif-controls {
  margin-top: 10px;
}

.gif-controls button {
  background: var(--primary-color);
  color: white;
  border: none;
  padding: 5px 10px;
  margin: 0 5px;
  border-radius: 3px;
  cursor: pointer;
}
  </style>
</head>
<body>
  <header>
    <h1>Autonomous Self-Driving Car Prototype</h1>
    <p class="meta">A complete hardware/software system with lane centering, traffic detection, and gesture control</p>
  </header>

  <section>
    <p>This project brings autonomous vehicle technology from research labs to your workspace, 
    implementing a functional self-driving car prototype with <strong>real-time lane keeping</strong>, 
    <strong>traffic light/sign detection</strong>, and <strong>multi-modal control interfaces</strong>.
    The system combines custom hardware with cutting-edge deep learning for a comprehensive autonomous driving experience.</p>
    <div class="image-container">
        <img src="{% static 'images/env.jpg' %}" width="100%">
        <div class="caption"> <strong></strong></div>
      </div>
    
    
  </section>

  <section>
    <h2>Hardware Architecture</h2>
    <p><strong>- It is a challenge in this project is to build a self-driving car that does not rely on any sensors like LiDAR or ultrasonic sensors,
         but instead depends solely on computer vision.</strong></p>
         <p><strong>- There is only one Camera at the center of the car, there is not any additional cameras or sensors.</strong></p>
    
    <div class="image-grid">
        <div>
          <img src="{% static 'images/ESP32-CAM2.jpg' %}" alt="Latency measurements">
          <div class="caption"><strong>Esp32-Cam</strong> </div>
        </div>
        <div>
          <img src="{% static 'images/l298n2.jpg' %}" alt="Steering error">
          <div class="caption"><strong>L298n motor driver</strong></div>
        </div>
        <div>
            <img src="{% static 'images/servo2.jpg' %}" alt="Latency measurements">
            <div class="caption"><strong>MG996r Servo motor</strong></div>
          </div>
          <div>
            <img src="{% static 'images/buck2.jpg' %}" alt="Latency measurements">
            <div class="caption"><strong>Buck Converter</strong></div>
          </div>
          <div>
            <img src="{% static 'images/dcmotor2.jpg' %}" alt="Latency measurements">
            <div class="caption"><strong>DC gear motor</strong></div>
          </div>
          <div>
            <img src="{% static 'images/lipo2.jpg' %}" alt="Latency measurements">
            <div class="caption"><strong>Power Supply 12V (3 lipo battery 3.7V)</strong></div>
          </div>
          <div>
            <img src="{% static 'images/chassis2.jpg' %}" alt="Latency measurements">
            <div class="caption"><strong>3D printed Chassis</strong></div>
          </div>
          <div>
            <img src="{% static 'images/track2.jpg' %}" alt="Latency measurements">
            <div class="caption"><strong>car closed-track</strong></div>
          </div>
          <div>
            <img src="{% static 'images/STM322.jpg' %}" alt="Latency measurements">
            <div class="caption"><strong>STM32 (F103C8T6 Blue Pill) </strong></div>
          </div>
      </div>
      <div class="image-container">
        <img src="{% static 'images/car.jpg' %}" width="50%">
        <div class="caption"> <strong>After getting all together</strong></div>
      </div>

    <h3>Core Components</h3>
    <div class="highlight-box">
      <ul>
        <li><strong>Chassis:</strong> Custom 3D printed design (modified from <a href="https://www.printables.com/model/447947-chassis-110-adaptable-dks-basic" 
            target="_blank"> Chassis 1/10 Adaptable - DKS-Basic model</a> )</li>
        <li><strong>Motors:</strong> Servo for steering + DC motor for propulsion</li>
        <li><strong>Vision System:</strong> ESP32-CAM for real-time video streaming</li>
        <li><strong>Processing:</strong> Laptop running AI models</li>
        <li><strong>Communication:</strong> WiFi network for low-latency control and communication between the laptop and the car.</li>
      </ul>
    </div>
  </section>

  <section>
    <h2>Key Features</h2>

    <h3>1. Lane Centering with Deep Learning</h3>
    <div class="image-container">
        <iframe width="100%" height="400" src="https://youtube.com/embed/XoIUX9xYDOE" frameborder="0" allowfullscreen></iframe>
      <div class="caption"><strong>Real-time lane centering using CNN predictions</strong></div>
    </div>

    <p><strong>Technical Implementation:</strong></p>
    <ol>
      <li><strong>Data Collection:</strong>
        <ul>
            <li>25 minutes of manual driving with synchronized frames/angles</li>
            <li>Data over sampling.</li>
          </ul> 
        </li>
        <div class="image-grid">
            <div>
              <img src="{% static 'images/data.jpg' %}" alt="Traffic light detection"  >
              <div class="caption">Training Data</div>
            </div>
            <div>
              <img src="{% static 'images/sample1.jpg' %}" alt="Sign detection"  >
              <div class="caption"></div>
            </div>
            <div>
                <img src="{% static 'images/sample2.jpg' %}" alt="Sign detection"  >
                <div class="caption"></div>
              </div>
           
          </div>
      <li><strong>CNN Architecture:</strong> Modified NVIDIA model with BatchNorm and Dropout</li>
      <ul>
        <li>optimizer: Adam</li>
        <li>Loss function: Mean Square Error MSE</li>
      </ul>
      <li><strong>Training:</strong> Progressive training strategy: 1500 epochs</li>
      <ul>
        <li>Phase 1 (500 epochs): Raw data only</li>
        <li>Phase 2 (1000 epochs): Augmented data (zoom, pan, brightness variance and flip)</li>
        <div class="image-container">
            <img src="{% static 'images/augImage.jpg' %}" width="100%">
            <div class="caption"> <strong></strong></div>
          </div>
      </ul>
      <li><strong>Preprocessing:</strong> Road ROI cropping, YUV conversion, and normalization</li>
      <div class="image-container">
        <img src="{% static 'images/imageProcess.jpg' %}" width="100%">
        <div class="caption"> <strong></strong></div>
      </div>
    </ol>

    <div class="highlight-box">
      <h4>Model Architecture</h4>
      
      <pre><code>def nvidia_model():
        model = Sequential()
        
        # Convolutional blocks with BatchNorm
        model.add(Conv2D(24, (5,5), strides=(2,2), input_shape=(66,200,3), activation='elu'))
        model.add(BatchNormalization())
        
        model.add(Conv2D(36, (5,5), strides=(2,2), activation='elu'))
        model.add(BatchNormalization())
        
        model.add(Conv2D(48, (5,5), strides=(2,2), activation='elu'))
        model.add(BatchNormalization())
        
        model.add(Conv2D(64, (3,3), activation='elu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.3))
        
        model.add(Conv2D(64, (3,3), activation='elu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.3))
        
        # Dense layers
        model.add(Flatten())
        model.add(Dense(100, activation='elu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.5))
        
        model.add(Dense(50, activation='elu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.3))
        
        model.add(Dense(10, activation='elu'))
        model.add(Dense(1))
        
        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
        model.compile(loss='mse', optimizer=optimizer)
        return model</code></pre>
    </div>
    <p><strong>Loss:</strong></p>
    <div class="image-container">
        <img src="{% static 'images/loss.jpg' %}" width="80%">
        <div class="caption"> <strong></strong></div>
      </div>


    <h3>2. Traffic Light/Sign Detection</h3>
    

    <ul>
      <li><strong>Dataset:</strong><a href="https://universe.roboflow.com/naseer-2uxt8/traffic-lights-and-signs-fvuv2" 
        target="_blank"> RoboFlow Traffic Lights and Signs</a>  (150 epochs)</li>
      <li><strong>Model:</strong> YOLOv12n with custom-trained weights</li>
      
    </ul>
    <div class="image-grid">
        <div>
          <img src="{% static 'images/light.jpg' %}" alt="Traffic light detection"  >
          <div class="caption">YOLOv12 detecting traffic lights</div>
        </div>
        <div>
          <img src="{% static 'images/stop.jpg' %}" alt="Sign detection"  >
          <div class="caption">Stop sign</div>
        </div>
       
      </div>
      <div class="image-container">
        <img src="{% static 'images/results.jpg' %}" alt="Gesture control interface">
        <div class="caption"></div>
      </div>
    <h3>3. Depth Estimation</h3>
    <div class="image-container">
      <img src="{% static 'images/depth.jpg' %}" alt="Gesture control interface">
      <div class="caption"></div>
    </div>

    <ul>
        <li><strong>Depth Estimation:</strong> <strong>Instead of relying on hardware-based distance sensors like LiDAR or ultrasonic,
            utilizing a deep learning-based monocular depth estimation model that estimates pixel-wise depth from single RGB frames,
           enabling real-time spatial awareness using only the onboard camera:</strong> </li>
        <li><strong>Model:</strong> DepthAnythingV2 </li>
        <li><strong>Object distance calculation for safe stopping</strong></li>
        
    </ul>
    <iframe width="100%" height="400" src="https://www.youtube.com/embed/JUzRpT87SU0" frameborder="0" allowfullscreen></iframe>


    <h3>4. Gesture & Voice Control</h3>
    <iframe width="100%" height="400" src="https://www.youtube.com/embed/kOgZX4lRixM" frameborder="0" allowfullscreen></iframe>
    <p><strong>Control Modes:</strong></p>
    <ul>
      <li><strong>Hand Gestures:</strong> OK sign (forward/stop), Rock (left/right), Fist (lights on/off)</li>
      <li><strong>Voice Commands:</strong> Supports both English and Arabic ("forward", "يمين", "stop")</li>
      <li><strong>Multi-threading:</strong> Parallel processing of vision and control inputs</li>
    </ul>
    <iframe width="100%" height="400" src="https://www.youtube.com/embed/NK6LTzw5TGE" frameborder="0" allowfullscreen></iframe>
  </section>

  <section>
    <h2>Technical Challenges & Solutions</h2>

    <div class="highlight-box">
      <h3>1. Real-time Performance</h3>
      <p><strong>Solution:</strong> Optimized pipeline with ESP32-CAM streaming and CUDA acceleration</p>
      
      <h3>2. Limited Training Data</h3>
      <p><strong>Solution:</strong> Advanced augmentation (zoom, pan, brightness, flip) and progressive training</p>
      
      <h3>3. Multi-modal Control</h3>
      <p><strong>Solution:</strong> Thread-safe queues for gesture/voice command processing</p>
    </div>

    
  </section>

  <section class="conclusion">
    <h2>Future Enhancements:</h2>
    
    
    <p><strong>LIDAR and ultrasonic sensors integration, use stereo camera for depth and 3D objecting, , more training tracks and roads,
         predicting throttle and speed parameters besides steering angle.</strong> </p>
  </section>

  <div class="button-container">
    <a href="https://github.com/osamazaki2525/Autonomous-self-driving-car" class="github-button" target="_blank" rel="noopener">
      <svg viewBox="0 0 16 16" width="16" height="16">
        <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
      </svg>
      GitHub
    </a>
    <!-- <a href="https://colab.research.google.com/drive/[your_colab_link]" class="colab-button" target="_blank" rel="noopener">
      <svg viewBox="0 0 256 256" width="16" height="16">
        <path d="M128 0a128 128 0 1 0 128 128A128.145 128.145 0 0 0 128 0Zm0 240a112 112 0 1 1 112-112 112.127 112.127 0 0 1-112 112Zm-12-136v72a12 12 0 0 1-24 0v-72a12 12 0 0 1 24 0Zm60 0v36.69l22.34-22.35a12 12 0 0 1 17 17l-22.35 22.34H188a12 12 0 0 1 0-24h.69L166 139.31V104a12 12 0 0 1 24 0Z" fill="#FFB800"/>
      </svg>
      Google Colab
    </a> -->
  </div>


</body>
</html>